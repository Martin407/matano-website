"use strict";(self.webpackChunk_matano_website=self.webpackChunk_matano_website||[]).push([[4125],{32647:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2022/03/09/regional-cloud-architecture","metadata":{"permalink":"/blog-archive/2022/03/09/regional-cloud-architecture","source":"@site/blog-archive/2022-03-09-regional-cloud-architecture/index.mdx","title":"How we build a fully regional cloud architecture on AWS","description":"How we\'ve built a fully regional cloud infrastructure on AWS.","date":"2022-03-09T00:00:00.000Z","formattedDate":"March 9, 2022","tags":[{"label":"engineering","permalink":"/blog-archive/tags/engineering"}],"readingTime":10.3,"hasTruncateMarker":true,"authors":[{"name":"Samrose Ahmed","url":"https://www.matano.dev","email":"samrose@matano.dev","imageURL":"https://github.com/Samrose-Ahmed.png","key":"samrose"}],"frontMatter":{"title":"How we build a fully regional cloud architecture on AWS","keywords":["software","cloud","AWS","regional"],"authors":["samrose"],"tags":["engineering"],"description":"How we\'ve built a fully regional cloud infrastructure on AWS."},"nextItem":{"title":"S3 POST Policy - The hidden S3 feature you haven\'t heard of","permalink":"/blog-archive/2022/02/14/s3-post-policy"}},"content":"import { Mermaid } from \\"mdx-mermaid/Mermaid\\";\\n\\nAt least for me, building on the cloud gives often gives me unexpected joy. To think that I can deploy servers in Bahrain or Japan from my couch is still something I get excited about!\\n\\nWe run completely on the cloud, specifically AWS, and one of the main reasons is how easy it is to launch services in new geographical regions. We deploy our services to independent cloud regions and provide our customers with regional endpoints (e.g. `events.us-west-2.publicapi.com`).\\n\\nHere are some of our learnings from building a fully regional service on AWS.\\n\\n\x3c!--truncate--\x3e\\n\\n## What are cloud regions?\\n\\nCloud providers have a concept of regions for their cloud. These are isolated geographical regions where their physical infrastructure is hosted and where they offer cloud services. For example, at the time of this article, AWS offers 26 different cloud regions. Software applications built on the cloud can leverage cloud regions to also build a concept of regions in their applications to improve their services.\\n\\n### Cellular architecture\\n\\nTo understand regionality, it\'s good to understand _cellular architecture_. Cellular (or cell-based) architecture is a way of separating systems into isolated _cells_ to reduce the blast radius from something going wrong. The idea is that a failure shouldn\'t ever cross cells, and so cellularization is a way of creating those independent partitions. The actual method of choosing a cell can be anything, from random based on ID to logical like geographical region. You can also have nested cells for further isolation (e.g. _Large volume customers in US West region_ could be a particular cell). AWS Availability zones, for example, are also sub-regional cells. All in all, cellularization is a very powerful way of designing available systems, you can [learn more about it here](https://www.youtube.com/watch?v=swQbA4zub20).\\n\\n### Why regions exist\\n\\nRegions, then, are just cells that are based on geographical region. Regions serve several purposes. From an availability and reliability perspective, regions are at the center of a cloud\'s reliability strategy. In a cellular architecture, regions serve as the first and most principal cells. They ensure that there is a logical containment of resources that are isolated from others. This helps prevent widespread outages.\\n\\nRegions do refer to actual geographic regions, so this is also an important part of why they exist. One benefit of this is latency. Providing a way for customers to ensure that their workloads run in a particular region helps them with running workloads close to their customers.\\n\\nAnother important usecase is compliance and data sovereignty. Many countries are passing legislation requiring user data and other sensitive data to be physically stored in locations under their jurisdiction. Cloud regions are a way to ensure compliance with such regulations.\\n\\n## Multi region architectures\\n\\nBecause cloud providers do the hard work of making sure all their services are available in isolated regions, one benefit customers get is being able to run multi region workloads. Multi region architecture is a way to get another layer of reliability beyond multi availability zone. You can, for example, run a load balanced service across multiple regions using Route 53 for latency based routing and health check based failover. One can also use regions for data preservation and backup, e.g. with Amazon S3. Region wide outages are pretty rare at AWS so active multi region for services is likely overkill (as it may also come with downsides like cross region data transfer), but everyone has their availability requirements and multi region architectures are a powerful tool to leverage for increased availability.\\n\\n## Regionalized architectures\\n\\nA regionalized architecture is one where the regionality is part of the interface of the service. For example, in regional architectures, one has dedicated regional endpoints that clients use for each supported region. Each regional service in a regional architecture should be independent of other instances running in other regions.\\n\\nAs an example, say we have an image processing API. Our clients use this API to upload, process, and retrieve videos. To regionalize this API, we can deploy the API to multiple regions and offer endpoints like `region1.videoapi.example.com` and `region2.videoapi.example.com` for our clients to use.\\n\\n#### Difference between multi region and regionalized architectures\\n\\n<div className=\\"cntr\\">\\n\\n![](./regional-diagram.png)\\n\\n_Comparison between regional (left) and multi region architectures_\\n\\n</div>\\n\\nThe key difference between multi region and regionalized architectures is that in regional architectures, the region is part of the contract of the service, whereas in multi region architectures it is solely part of the implementation. In a multi region architecture, regions are used as cells for increased availability or as a failover. There is no guarantee that a specific customer\'s requests will run in a specific region. Rather, the customer doesn\'t even have to know about the concept of region vis-\xe0-vis the service. In a regional architecture, however, the region is part of the public API, and customers choose or are informed of the region they are associated with.\\n\\n### Choosing regions for a regional architecture\\n\\nWhen building a regionalized architecture, one needs to pick a list of geographical regions to support and a methodology to assign them. You\'re likely deployed on a cloud, so a simple decision is to choose your regions to correspond to cloud regions. This is what we do, for example. One can rename them, or just use the same names, which is what we do ourselves.\\n\\nHowever, a one-to-one mapping is not necessary. You can also come up with regions that correspond to multiple cloud regions. For example, a region structure could look as follows:\\n\\n<div className=\\"cntr\\">\\n\\n| Region |          AWS Region(s)          |\\n| :----: | :-----------------------------: |\\n|   us   | us-east-1, us-west-2, us-east-2 |\\n|   eu   |      eu-west-1, eu-west-2       |\\n|   jp   | ap-northeast-1, ap-northeast-2  |\\n|   in   |     ap-south-1, ap-south-2      |\\n\\n_Regionalization scheme using multi region cells_\\n\\n</div>\\n\\nSuch a scheme gives one some benefit of each application region consisting of multiple cloud regions, which can improve availability.\\n\\n## When to use a regional architecture\\n\\nMost services likely don\'t need to be fully regionalized. Unlike multi-regionality, which is solely a technical engineering decision, choosing to regionalize your public APIs or services is more so a product decision. It should generally serve some need or purpose for your customer. We observe common usecases:\\n\\n#### Data residency requirements\\n\\nRegionalization of your services is a way of allowing your customers to ensure their data is stored and processed in a specific region. This is often requested due to regulatory reasons.\\n\\n#### Latency\\n\\nFor other applications, minimizing client latency is highly important. Keeping requests in the region closest to a customer is a way to achieve this. However, as keeping requests in region is not inherently a strict requirement, complete regionalization may or may not be the correct approach here, as a single endpoint with latency based routing may also fulfill the requirements. The desired experience of the customer is important to consider here (e.g. do you want your customer to have to think of regions?).\\n\\n## Choosing components to regionalize\\n\\nWhen evaluating a regional architecture, a natural question is what components to regionalize. Specifically, which services to have a publicly regional interface for. There are several considerations here, and the answer goes back to why one is using a regional architecture in the first place. A useful heuristic we use is one between _control planes_ and _data planes_. Generally, control plane services shouldn\'t be regional, but dataplane services can be. To understand why, we may consider the common actions we do in the control plane: e.g. billing, user management, or other global configuration. These often have single region dependencies and are relatively infrequent. For us, it\'s neither important nor desirable to have these be regionalized. On the other hand, our data plane processes that process and store important data are regionalized.\\n\\nNote that while this is a general guideline that we ourselves have found useful, it\'s in no way prescriptive. There are many cases where one may make the control plane regionalized as well. In general, the reason for regionalization should be central to this decision. One should consider how regionalizing a service will help or prevent them from achieving that goal.\\n\\n## How we deploy to many regions\\n\\nInfrastructure and deployment automation is key to being able to maintain consistency, reliability, and understandability while deploying many services across different regions. Thankfully, infrastructure as code solutions, particularly the AWS CDK, make this much easier.\\n\\n### Staged deployment using Waves\\n\\nWe use a concept of _waves_ when deploying software. Going back to cellular architecture, a wave is essentially a set of cells to deploy to concurrently. For example, currently, our waves are:\\n\\n<div className=\\"cntr\\">\\n\\n|  Wave  |   Regions   |\\n| :----: | :------------------: |\\n| Wave 1 |      ap-south-1      |\\n| Wave 2 |      eu-west-1       |\\n| Wave 3 | us-west-2, us-east-1 |\\n\\n_Our current waves_\\n\\n</div>\\n\\nDeploying to one wave at a time ensures that faulty changes don\'t cause global outages. Combined with bake times (adding wait times between waves to monitor for degradation) and automated rollbacks, waves can help ensure that you don\'t have catastrophic failures. We\'ve found them to be very useful for staging our changes. You can [learn more about waves here](https://aws.amazon.com/builders-library/automating-safe-hands-off-deployments/).\\n\\n### Infrastructure as Code - CDK\\n\\nCompletely automating your infrastructure is essential when deploying to many regions since one is duplicating one\'s entire application and services each time they deploy to a new region. We use the AWS CDK to help us do this. CDK is a wrapper around CloudFormation that lets you write your infrastructure as real code (e.g. we use Typescript). It makes building reusable abstractions (called _constructs_) as easy as writing a class or a function ([Learn more about it here](https://docs.aws.amazon.com/cdk/v2/guide/home.html), we think it\'s one of the coolest things AWS offers!).\\n\\nThe CDK also comes with useful high level abstractions out of the box, so you don\'t have to reinvent the wheel.\\n\\n<div className=\\"cntr\\">\\n\\n![](./waves.png)\\n\\n_A pipeline deploying one of our regional services to one of our waves_\\n\\n</div>\\n\\nFor example, we use [CDK Pipelines](https://aws.amazon.com/blogs/developer/cdk-pipelines-continuous-delivery-for-aws-cdk-applications) for deploying all of our infrastructure. In CDK Pipelines, stages and _waves_ are supported natively so you can easily deploy cellular applications. We\'ve developed a standard _Pipeline_ construct that sets up our standard waves and makes creating a regionalized service conforming to regions and waves very simple.\\n\\n## Other considerations around regional architectures\\n\\nHere are some other things we\'ve learned building regional applications on AWS.\\n\\n#### Use separate AWS accounts per region\\n\\nThis is general AWS best practice but reinforces thinking of each instance of a service deployed in a region as separate with clearly delineated blast radiuses.\\n\\n#### Managing cost\\n\\nRegionalization inherently comes with some additional cost. However, when an application is architected correctly, the additional cost due to regionalization should mainly be a fixed cost per region, which is generally negligible at scale. Regionalized services shouldn\'t generally have excessive have cross-region data transfer (databases and other global data is often the exception, but this should be low cost).\\n\\n#### Global data\\n\\nA common consideration when evaluating a regionalized or multi region architecture is what to do with global data. There\'s no one answer here. Keeping in mind our earlier discussion on choosing components to regionalize and data planes & control planes, there are several ways we could deal with this. For example, we could keep global data in one control plane region, and our regionalized data plane services can depend on this control plane. This is what we use ourselves. However, this may or may not be acceptable. If latency is the primary motivation for regionalization, then this is less than ideal. There are several ways to deal with this, including DynamoDB Global Tables and database replication, that are beyond the scope of this article (see this [video](https://www.youtube.com/watch?v=2e29I3dA8o4) for some more information). On the other hand, if the main reason for regionalization is data residency, and our control plane only stores non-sensitive configuration, then this is a more fitting approach.\\n\\n## Conclusion\\n\\nAs it gets easier and easier to deploy to the cloud, and with increased concern about data sovereignty, regional architectures are becoming more common. This gave you an overview of how we easily maintain our regional services on AWS. You might not need regionalization for your next project, but I hope this was helpful and informative.\\n\\n_What do you think of regionalization, do you use it in your applications? Feel free to reply._"},{"id":"/2022/02/14/s3-post-policy","metadata":{"permalink":"/blog-archive/2022/02/14/s3-post-policy","source":"@site/blog-archive/2022-02-14-s3-post-policy/index.mdx","title":"S3 POST Policy - The hidden S3 feature you haven\'t heard of","description":"A look at S3 POST Policies and how to use them to create secure, short lived client object upload sessions.","date":"2022-02-14T00:00:00.000Z","formattedDate":"February 14, 2022","tags":[{"label":"engineering","permalink":"/blog-archive/tags/engineering"}],"readingTime":7.28,"hasTruncateMarker":true,"authors":[{"name":"Samrose Ahmed","url":"https://www.matano.dev","email":"samrose@matano.dev","imageURL":"https://github.com/Samrose-Ahmed.png","key":"samrose"}],"frontMatter":{"title":"S3 POST Policy - The hidden S3 feature you haven\'t heard of","keywords":["s3","aws","post policy"],"authors":["samrose"],"tags":["engineering"],"description":"A look at S3 POST Policies and how to use them to create secure, short lived client object upload sessions."},"prevItem":{"title":"How we build a fully regional cloud architecture on AWS","permalink":"/blog-archive/2022/03/09/regional-cloud-architecture"}},"content":"import { Mermaid } from \\"mdx-mermaid/Mermaid\\";\\n\\nSay you\'re building an application and you need to let your users upload files to S3. How would you go about it?\\n\\n\x3c!--truncate--\x3e\\n\\nParticularly, imagine our clients are uploading a lot of files of different sizes, and are sensitive to latency.\\n\\nLet\'s walk through a journey of AWS APIs, and explore a little known feature of S3 called _POST Policies_.\\n\\n## Presigned URLs\\n\\nYour immediate instinct may be to use [S3 presigned URLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/PresignedUrlUploadObject.html).\\nPresigned URLs let you create a URL that you can share and allow a user to download or upload to an S3 bucket. You create the presigned\\nURL server side using IAM credentials that have the valid S3 permissions and then share the URL to allow user actions. Clients simply use HTTP clients to connect to the URL.\\nYou can set an expiry time to any date time to ensure the access is short lived and you can also attach an IAM policy to the presigned URL to limit the permissions the client has.\\n\\nAll in all, presigned URLs are pretty powerful and sound like a great choice for allowing credential-less S3 actions. They have one major limitation, however.\\n_S3 presigned upload urls require you to know the Content length before hand._ Because of the way presigned URLs work, using AWS4 Signatures, the Content-Length\\nis a required component when generating the presigned URL. This means we can\'t return one presigned URL and allow our clients to upload objects of variable size while\\nit is not expired.\\n\\nAs a workaround, we can have the client request a presigned URL every time they want to perform an upload. This is not necessarily a big deal, and may\\nbe perfectly suitable for many scenarios.\\n\\n<Mermaid\\n  chart={`%%{init: {\'theme\': \'base\', \'themeVariables\': { \'fontSize\': \'13px\', \'fontFamily\': \'Inter\'}}}%%\\n  sequenceDiagram\\n    loop Each upload\\n      Client->>Server: Request presigned URL\\n      Server->>Client: Presigned  URL\\n      Client->>S3: Upload object\\n    end`}\\n/>\\n\\nHowever, this results in an additional call for every upload and may not be desirable. For example, say we want our clients to have a short lived\\nsession where they can upload a large number of objects, and latency is important.\\n\\n## A new API using temporary credentials\\n\\nPresigned URLs don\'t seem to work well for our usecase, so let\'s try a new approach. AWS IAM offers APIs to\\n[request temporary security credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html). There\'s\\na few different APIs, but let\'s see if we can use [AssumeRole](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html) to return temporary\\ncredentials to our client.\\n\\nAs an approach, we can call STS AssumeRole server side and return temporary IAM credentials to our client. We can use IAM [session policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_session)\\nto limit the S3 permissions the client has access to. We can also use the [DurationSeconds](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html#API_AssumeRole_RequestParameters)\\nparameter to limit the validity of the credentials, but only up to a minimum of 15 minutes.\\n\\nOur clients would then use the credentials and upload files using the AWS SDK. If we\'re offering this as a part of our API, we\'d likely want to write a language native client\\nthat wraps the AWS SDK and takes care of refereshing the credentials.\\n\\nSecurity-wise, you may feel icky having to return credentials using your API, but the approach is generally sound as long as the credentials are short lived and you are\\ngiving access to authenticated callers with narrow permissions.\\n\\nThis approach works, but besides having to implement this API, comes with several downsides:\\n\\n- Dealing with unsigned credentials.\\n- Our client needs to depend on heavyweight AWS SDKs, rather than a simple HTTP Client.\\n- We can\'t control the size of the object our client uploads. This can be a concern with untrusted or semi-trusted clients, who could upload very\\n  large files to our S3 bucket.\\n\\n## Enter POST Policy\\n\\nWe\'re dissatisfied with our previous approach. Let\'s explore a lesser known Amazon S3 feature: [POST Policy](https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-HTTPPOSTConstructPolicy.html).\\nYou might be thinking, _\\"POST?, isn\'t it S3 PUT object?\\"_, and you\'re right, but Amazon actually introduced a POST API for uploading S3 objects to enable browser based S3 uploads.\\n\\nAs a note, you\'ll generally hear of POST Policy in the context of browser based uploads, but there\'s nothing inherent preventing us from using it in any environment.\\n\\nA POST policy is essentially a JSON document that you create, sign, and return to your clients to specify what conditions are required for a successful POST object upload.\\n\\n#### What does a POST policy look like?\\n\\nPOST policies are fairly powerful: you can specify the exact date time the policy expires and include conditions on properties like the _ACL_, _Bucket_, _Key prefix_,\\nand _Content length range_ (minimum and maximum). You can also use operators like \\"starts_with\\" in addition to exact matches to add dynamic logic to your policy.\\n\\nLet\'s take a look at an example POST policy.\\n\\n```json\\n{\\n  \\"expiration\\": \\"2022-02-14T13:08:46.864Z\\",\\n  \\"conditions\\": [\\n    { \\"acl\\": \\"bucket-owner-full-control\\" },\\n    { \\"bucket\\": \\"my-bucket\\" },\\n    [\\"starts-with\\", \\"$key\\", \\"stuff/clientId\\"],\\n    [\\"content-length-range\\", 1048576, 10485760]\\n  ]\\n}\\n```\\n\\nThe policy results in the following conditions:\\n\\n- The policy expires on Mon Feb 14 2022 13:08:46 UTC. After this time, any client using the policy to perform a POST upload will get a 403 error.\\n- The ACL on the object must be `Bucket owner full control`, which ensures we, as the bucket owner, have full control of uploaded objects.\\n- We specify a specific bucket by name (`my-bucket`) to allow uploads to.\\n- The S3 key of the uploaded object must have a specific prefix. Here, we use it ensure our client only has permission to upload under their prefix by specifying their client ID as the key prefix.\\n- The uploaded object must be between 1MB and 10MB in size.\\n\\n#### Signing the policy\\n\\nAfter forming a POST policy, we have to _sign_ the policy using valid IAM credentials (with the requisite permissions), similar to how we sign presigned URLs. You can view the\\ncomplete procedure on calculating the signature [here](https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-UsingHTTPPOST.html#sigv4-post-signature-calc) but it\\nessentially involves Base64 encoding the policy and signing it using AWS SigV4. Unfortunately, unlike presigned URLs, the AWS SDKs don\'t provide helper methods\\nto create the POST Policy. You can write it yourself or consult the few examples and community libraries out there. If you\'re using Java/JVM, check out\\n[Minio\'s implementation](https://github.com/minio/minio-java/blob/master/api/src/main/java/io/minio/PostPolicy.java) as a well maintained reference.\\n\\n#### Letting our clients perform POST uploads\\n\\nOnce we\'ve created the POST policy, our client\'s can use the POST policy to perform POST S3 uploads. S3 POST uploads are multipart form data requests to the S3 Bucket URL\\n(e.g. `https://examplebucket.s3-us-west-2.amazonaws.com/`) containing the key\'s specified in the POST policy. As a code example in Python:\\n\\n```python\\nfrom uuid import uuid\\nimport requests\\n\\n# Add an endpoint for the client to request a POST Policy\\npost_policy_form_data = requests.get(\\"/postPolicyFormData\\")\\npost_policy_form_data = {\\n  \\"x-amz-date\\": \\"20220213T233352Z\\",\\n  \\"x-amz-signature\\": \\"efa9bbc<...>\\",\\n  \\"acl\\": \\"bucket-owner-full-control\\",\\n  \\"x-amz-security-token\\": \\"<...>\\",\\n  \\"x-amz-algorithm\\": \\"AWS4-HMAC-SHA256\\",\\n  \\"x-amz-credential\\": \\"ASIA<..>.\\",\\n  \\"policy\\": \\"eyJleHBpcmF...<base64 encoded policy>\\",\\n  \\"Content-Type\\": \\"application/json\\"\\n}\\n\\nfilename = str(uuid4()) + \\".json\\"\\nkey = os.path.join(\\"client_id\\", filename)\\ncontent = \\"file_content\\"\\nmultipart_form_data = { **post_policy_form_data, \\"key\\": key, \\"file\\": (filename, content) }\\n\\nupload_url = \\"https://examplebucket.s3-us-west-2.amazonaws.com\\"\\nres = requests.post(upload_url, files=multipart_form_data)\\n```\\n\\nPOST policies satisfy all of our requirements.\\n\\n- We are using signed policies without raw credentials.\\n- Our clients can make HTTP requests without the AWS SDK.\\n- We can granularly control the expiration, permissions, and object properties,\\n- Our clients can upload objects as long as the POST policy is not expired without needing to make additional requests.\\n\\n## Conclusion\\n\\nWe took a short look at the S3 object upload landscape, and discovered a powerful feature called POST Policies.\\n\\n### Usecases\\n\\nLets take a look at some usecases that POST policies unlock:\\n\\n#### Browser based uploads\\n\\nParticularly for large sized files, POST policies provide a convenient way to enable your client\'s to upload files client side, without needing to go through a server proxy.\\nThis can be convenient if you\'re using API Gateway or Lambda which have content size limits. Additionally, this can give your client better upload speed.\\n\\n#### Short lived low-latency upload sessions\\n\\nAs we discussed, we can use POST policies to let our clients maintain a short lived, controlled session with specific permissions where they can upload many objects\\nof variable size, without any additional latency besides S3 latency.\\n\\n### Guidance\\n\\nAs a takeway, if you are looking to incorporate S3 object upload from your clients in your application, follow the general guidance:\\n\\n- Prefer presigned URLs. They\'re simpler, are already supported in the AWS SDKs and are well documented.\\n- Use POST policies otherwise. As we discussed, if latency is important, or you want form based browser uploads.\\n- Don\'t use the second approach we discussed (using `AssumeRole`). You can generally achieve the equivalent using a POST policy.\\n\\nAre you using POST policies, or do you know of an interesting usecase they enable? Feel free to reply."}]}')}}]);